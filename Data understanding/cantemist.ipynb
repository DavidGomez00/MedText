{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a3ba208",
   "metadata": {},
   "source": [
    "# Estructura de anotaciones en CANTEMIST\n",
    "\n",
    "En cantemist-norm los datos se presentan de la siguiente manera:\n",
    "- nota1.txt\n",
    "- nota1.ann <- Fichero de anotaci√≥n\n",
    "\n",
    "La anotaci√≥n viene en un fichero de texto por filas. Cada dos filas est√°n organizadas de la siguiente forma:\n",
    "La primera fila contiene esta informaci√≥n (sin header)\n",
    "\n",
    "| indice t√©rmino | categoria | start char offset | end char offset | mention string |\n",
    "|----|-----|-----|-----|----|\n",
    "| T1 | MORFOLOGIA_NEOPLASIA | 3332 | 3341 | Carcinoma miocr√≠tico |\n",
    "\n",
    "La segunda fila tiene esta informaci√≥n (sin header)\n",
    "| indice | Annotator Notes | indice t√©rmino | eCIE-O 3.1 code |\n",
    "|----|----|----|----|\n",
    "| #1| AnnotatorNotes | T1 | 8041/3 |\n",
    "\n",
    "La anotaci√≥n en la segunda fila se corresponde al t√©rmino descrito en la primera. Para parsear estas anotaciones a un formato m√°s legible he implementado la siguiente celda con la funci√≥n `parse_cantemist_annotation()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f2ca8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def parse_cantemist_annotation(ann_path):\n",
    "    ''' Parsea anotaciones de Cantemist para hacer un dataframe estructurado.\n",
    "    \n",
    "        Input: Un fichero de anotaci√≥n de cantemist. \n",
    "        Output: Un dataframe con la anotaci√≥n estructurada.\n",
    "    '''\n",
    "    # Comprueba que el fichero no est√© vac√≠o\n",
    "    if os.path.getsize(ann_path) == 0:\n",
    "        return pd.DataFrame(columns=['term_idx', 'text', 'category', 'char_start', 'char_end', 'ICD-O Code'])\n",
    "    \n",
    "    # Lee el fichero de anotaci√≥n\n",
    "    ann = pd.read_csv(\n",
    "        ann_path,\n",
    "        sep = '\\t',\n",
    "        dtype=str,\n",
    "        encoding='utf-8',\n",
    "        header=None\n",
    "    )\n",
    "\n",
    "    # Parsea la parte de las coordenadas\n",
    "    nota_coords = ann[ann[0].str.startswith('T')].copy()\n",
    "    split = nota_coords[1].str.split(' ', expand=True)\n",
    "    nota_coords = pd.concat([nota_coords[[0, 2]], split], axis=1)\n",
    "    nota_coords.columns = ['term_idx', 'text', 'category', 'char_start', 'char_end']\n",
    "\n",
    "    # Parsea las filas de anotaci√≥n\n",
    "    nota_ann = ann[ann[0].str.startswith('#')].copy()\n",
    "    nota_ann[0] = nota_ann[0].str.replace('#', 'T')\n",
    "    nota_ann = nota_ann.drop(1, axis=1)\n",
    "    nota_ann.columns = ['term_idx', 'ICD-O Code']\n",
    "\n",
    "    # Merge de las dos partes\n",
    "    merged = pd.merge(nota_coords, nota_ann, on='term_idx', how='inner')\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54544e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [term_idx, text, category, char_start, char_end, ICD-O Code]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../.data/Cantemist/dev-set1/cantemist-norm/\"\n",
    "nota = \"cc_onco853.ann\"\n",
    "\n",
    "df = parse_cantemist_annotation(data_path + nota)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3a35c6",
   "metadata": {},
   "source": [
    "# Frecuencia de t√©rminos\n",
    "Ahora que s√© parsear las anotaciones de Cantemist, quiero ver cu√°l es el t√©rmino m√°s frecuente en las notas. Para ello voy a leer todas las notas de .data/Cantemist/dev-set1/cantemist-norm/ e ir anotaci√≥n por anotaci√≥n contando los c√≥digos ICD-O que aparecen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90ec8cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "data_path = '../.data/Cantemist/dev-set1/cantemist-norm/'\n",
    "\n",
    "dict_counts = {}\n",
    "\n",
    "for nota in os.listdir(data_path):\n",
    "    if nota.endswith('.ann'):\n",
    "        df = parse_cantemist_annotation(data_path + nota)\n",
    "        \n",
    "        # Cuenta el n√∫mero de veces que aparece cada c√≥digo ICD-O\n",
    "        code_counts = df['ICD-O Code'].value_counts()\n",
    "        for code, count in code_counts.items():\n",
    "            if code in dict_counts:\n",
    "                dict_counts[code] += count\n",
    "            else:\n",
    "                dict_counts[code] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2230134e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ICD-O Code  Count                                       ICD-O string\n",
      "0      8000/6   1220                               Neoplasm, metastatic\n",
      "1      8000/1    589    Neoplasm, uncertain whether benign or malignant\n",
      "2      8000/3    242                                Neoplasm, malignant\n",
      "3      8140/3     88                                Adenocarcinoma, NOS\n",
      "4      8500/3     52                   Infiltrating duct carcinoma, NOS\n",
      "5      8010/3     46                                     Carcinoma, NOS\n",
      "6      8140/6     41                    Adenocarcinoma, metastatic, NOS\n",
      "7      8720/3     33                            Malignant melanoma, NOS\n",
      "8      8001/1     29  Tumor cells, uncertain whether benign or malig...\n",
      "9      8001/3     25                             Tumor cells, malignant\n",
      "10     8010/9     23                                     Carcinomatosis\n",
      "11     8720/6     22                                                NaN\n",
      "12     8010/6     21                         Carcinoma, metastatic, NOS\n",
      "13     8170/3     20                      Hepatocellular carcinoma, NOS\n",
      "14     9064/3     18                                          Germinoma\n",
      "15     8070/3     16                       Squamous cell carcinoma, NOS\n",
      "16     8240/3     14                               Carcinoid tumor, NOS\n",
      "17     8800/3     14                                       Sarcoma, NOS\n",
      "18    8140/33     13                                                NaN\n",
      "19     9260/3     13                                      Ewing sarcoma\n"
     ]
    }
   ],
   "source": [
    "# Convierte el diccionario a un dataframe para visualizarlo mejor\n",
    "counts_df = pd.DataFrame(list(dict_counts.items()), columns=['ICD-O Code', 'Count'])\n",
    "counts_df = counts_df.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Carga el mapa de c√≥digos a t√©rminos\n",
    "ecie_umls_map = pd.read_csv(\n",
    "    \"../.data/mappings/ICD-O-3.1-NCIt_Morphology_Mapping.txt\",\n",
    "    sep = '\\t',\n",
    "    dtype=str,\n",
    "    encoding='utf-8'\n",
    ")\n",
    "\n",
    "# A√±ade a cada c√≥digo su t√©rmino preferido\n",
    "counts_df = counts_df.merge(\n",
    "    ecie_umls_map[ecie_umls_map['Term Type'] == 'PT'][['ICD-O Code', 'ICD-O string']],\n",
    "    on='ICD-O Code',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(counts_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6df32c9",
   "metadata": {},
   "source": [
    "Una vez tenemos el t√©rmino que queremos buscar, en este caso \"Adenocarcinoma, metastasic, NOS\" (C√≥digo 8140/3), buscamos sus sin√≥nimos para aumentar el texto a encontrar en las notas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4003f3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sin√≥nimos para el c√≥digo 8140/6: []\n",
      "Sin√≥nimos para el c√≥digo 8000/6: ['Secondary Neoplasm', 'Tumor Embolism', 'Metastatic Neoplasm', 'Secondary Neoplasm']\n"
     ]
    }
   ],
   "source": [
    "def find_synonims(code):\n",
    "    '''Funci√≥n para encontrar los sin√≥nimos de un c√≥digo ICD-O.\n",
    "        Input: c√≥digo ICD-O (str).\n",
    "        Output: lista de sin√≥nimos (list of str).\n",
    "    '''\n",
    "    \n",
    "    # Carga el mapa de c√≥digos a t√©rminos\n",
    "    ecie_umls_map = pd.read_csv(\n",
    "        \"../.data/mappings/ICD-O-3.1-NCIt_Morphology_Mapping.txt\",\n",
    "        sep = '\\t',\n",
    "        dtype=str,\n",
    "        encoding='utf-8'\n",
    "    )\n",
    "    \n",
    "    filtered = ecie_umls_map[(ecie_umls_map['ICD-O Code'] == code) & (ecie_umls_map['Term Type'] == 'SY')]\n",
    "    \n",
    "    # Si no tiene sin√≥nimos, devuelve una lista vac√≠a\n",
    "    if filtered.empty:\n",
    "        return []\n",
    "    \n",
    "    synonims = filtered['NCIt PT string (Preferred term)'].tolist()\n",
    "    return synonims\n",
    "\n",
    "# Ejemplo de uso\n",
    "example_code = counts_df.iloc[6]['ICD-O Code']\n",
    "syns = find_synonims(example_code)\n",
    "print(f'Sin√≥nimos para el c√≥digo {example_code}: {syns}')\n",
    "\n",
    "example_code = counts_df.iloc[0]['ICD-O Code']\n",
    "syns = find_synonims(example_code)\n",
    "print(f'Sin√≥nimos para el c√≥digo {example_code}: {syns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e738d1c4",
   "metadata": {},
   "source": [
    "# N√∫mero de tokens por nota\n",
    "Para asegurarme de que no me quedo sin espacio analizando las notas, voy a medir el n√∫mero de tokens de cada una."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "afc67111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def graficar_distribucion_tokens(token_counts, dataset_name:str, output_file:str=\"distribucion_tokens.png\"):\n",
    "    \"\"\" Genera un histograma con la distribuci√≥n de tokens y marca los l√≠mites de contexto.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(token_counts) == 0:\n",
    "        print(\"No hay datos para graficar.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # 1. Crear el Histograma\n",
    "    # 'bins=50' divide tus datos en 50 barras para ver el detalle\n",
    "    plt.hist(token_counts, bins=50, color='#4C72B0', edgecolor='black', alpha=0.7, label='Frecuencia de Notas')\n",
    "    \n",
    "    # 2. L√≠neas de referencia estad√≠stica\n",
    "    media = np.mean(token_counts)\n",
    "    p95 = np.percentile(token_counts, 95)\n",
    "    \n",
    "    plt.axvline(media, color='red', linestyle='dashed', linewidth=1.5, label=f'Media ({int(media)})')\n",
    "    plt.axvline(p95, color='purple', linestyle='dashed', linewidth=1.5, label=f'Percentil 95% ({int(p95)})')\n",
    "    \n",
    "    # 3. L√≠neas de Context Window (Las fronteras cr√≠ticas)\n",
    "    # Estos son los saltos t√≠picos en configuraci√≥n de LLMs\n",
    "    context_limits = [2048, 4096, 8192, 16384]\n",
    "    colors = ['orange', 'green', 'brown', 'gray']\n",
    "    \n",
    "    max_val = np.max(token_counts)\n",
    "    \n",
    "    for limit, color in zip(context_limits, colors):\n",
    "        # Solo dibujamos la l√≠nea si est√° dentro del rango visual o cerca\n",
    "        if limit < max_val * 1.2: \n",
    "            plt.axvline(limit, color=color, linestyle='dotted', linewidth=2, label=f'L√≠mite {limit//1024}k')\n",
    "\n",
    "    # 4. Est√©tica\n",
    "    plt.title(f'Distribuci√≥n de Longitud de Tokens en {dataset_name}', fontsize=14)\n",
    "    plt.xlabel('N√∫mero de Tokens', fontsize=12)\n",
    "    plt.ylabel('Cantidad de Documentos', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 5. Guardar\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300)\n",
    "    print(f\"üìä Gr√°fico guardado exitosamente en: {output_file}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def analizar_dataset(dataset_path:str, dataset_name:str, model_id:str):\n",
    "\n",
    "    print(f\"‚¨áÔ∏è  Cargando tokenizador de {model_id}...\")\n",
    "    try:\n",
    "        # Solo descarga el vocabulario (< 2MB usually), no el modelo entero\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    except Exception as e:\n",
    "        print(f\"Error cargando tokenizador: {e}\")\n",
    "        return\n",
    "    \n",
    "    search_pattern = os.path.join(dataset_path, \"**/*.txt\")\n",
    "    files = glob.glob(search_pattern, recursive=True)\n",
    "\n",
    "    if not files:\n",
    "        print(f\"‚ùå No se encontraron archivos .txt en {dataset_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìÇ Se encontraron {len(files)} documentos. Analizando...\")\n",
    "    \n",
    "    token_counts = []\n",
    "    \n",
    "    for file_path in files:\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "                \n",
    "            # Esta es la magia: codificar el texto a IDs de tokens y contar\n",
    "            # Nota: No necesitamos attention_mask para esto\n",
    "            tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "            count = len(tokens)\n",
    "            token_counts.append(count)\n",
    "            \n",
    "            # Opcional: Imprimir los archivos gigantes para inspeccionarlos luego\n",
    "            if count > 6000: \n",
    "                print(f\"‚ö†Ô∏è  Archivo grande detectado ({count} tokens): {os.path.basename(file_path)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error leyendo {file_path}: {e}\")\n",
    "\n",
    "    # --- ESTAD√çSTICAS ---\n",
    "    token_counts = np.array(token_counts)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"üìä RESULTADOS DEL AN√ÅLISIS ({model_id})\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Total documentos: {len(token_counts)}\")\n",
    "    print(f\"üü¢ M√≠nimo:          {np.min(token_counts)} tokens\")\n",
    "    print(f\"üü° Media (Promedio): {int(np.mean(token_counts))} tokens\")\n",
    "    print(f\"üü† Mediana:         {int(np.median(token_counts))} tokens\")\n",
    "    print(f\"üî¥ M√°ximo:          {np.max(token_counts)} tokens\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Percentil 90:       {int(np.percentile(token_counts, 90))} tokens (El 90% de tus notas miden menos que esto)\")\n",
    "    print(f\"Percentil 95:       {int(np.percentile(token_counts, 95))} tokens\")\n",
    "    print(f\"Percentil 99:       {int(np.percentile(token_counts, 99))} tokens\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    # --- RECOMENDACI√ìN AUTOM√ÅTICA ---\n",
    "    max_tokens = np.max(token_counts)\n",
    "    prompt_estimate = 500  # Espacio para tus instrucciones de sistema\n",
    "    output_estimate = 2000 # Espacio para el JSON de salida\n",
    "    \n",
    "    needed_ctx = max_tokens + prompt_estimate + output_estimate\n",
    "    \n",
    "    print(\"\\nüí° RECOMENDACI√ìN DE INGENIER√çA:\")\n",
    "    print(f\"Para cubrir el CASO PEOR (Nota m√°s larga + Prompt + Salida):\")\n",
    "    print(f\"Necesitas: {max_tokens} (Nota) + {prompt_estimate} (Sys) + {output_estimate} (Output) = {needed_ctx} tokens\")\n",
    "    \n",
    "    standard_sizes = [2048, 4096, 8192, 16384, 32768]\n",
    "    recommended = next((x for x in standard_sizes if x >= needed_ctx), \"32k+\")\n",
    "    \n",
    "    print(f\"üëâ Configura 'num_ctx' en Ollama a: {recommended}\")\n",
    "\n",
    "    graficar_distribucion_tokens(token_counts, dataset_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2fd86123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚¨áÔ∏è  Cargando tokenizador de Qwen/Qwen2.5-7B-Instruct...\n",
      "üìÇ Se encontraron 8536 documentos. Analizando...\n",
      "‚ö†Ô∏è  Archivo grande detectado (9200 tokens): S1130-01082008001200008-1.txt\n",
      "‚ö†Ô∏è  Archivo grande detectado (6735 tokens): S1130-52742010000200007-1.txt\n",
      "‚ö†Ô∏è  Archivo grande detectado (7189 tokens): S1887-85712013000300006-1.txt\n",
      "‚ö†Ô∏è  Archivo grande detectado (6064 tokens): S0211-57352013000400004-1.txt\n",
      "‚ö†Ô∏è  Archivo grande detectado (7429 tokens): S1130-01082007000500007-1.txt\n",
      "\n",
      "========================================\n",
      "üìä RESULTADOS DEL AN√ÅLISIS (Qwen/Qwen2.5-7B-Instruct)\n",
      "========================================\n",
      "Total documentos: 8536\n",
      "üü¢ M√≠nimo:          30 tokens\n",
      "üü° Media (Promedio): 989 tokens\n",
      "üü† Mediana:         871 tokens\n",
      "üî¥ M√°ximo:          9200 tokens\n",
      "----------------------------------------\n",
      "Percentil 90:       1855 tokens (El 90% de tus notas miden menos que esto)\n",
      "Percentil 95:       2158 tokens\n",
      "Percentil 99:       2942 tokens\n",
      "========================================\n",
      "\n",
      "üí° RECOMENDACI√ìN DE INGENIER√çA:\n",
      "Para cubrir el CASO PEOR (Nota m√°s larga + Prompt + Salida):\n",
      "Necesitas: 9200 (Nota) + 500 (Sys) + 2000 (Output) = 11700 tokens\n",
      "üëâ Configura 'num_ctx' en Ollama a: 16384\n",
      "üìä Gr√°fico guardado exitosamente en: distribucion_tokens.png\n"
     ]
    }
   ],
   "source": [
    "token_counts = analizar_dataset(\n",
    "    dataset_path=\"/home/david/GitHub/MedText/.data/Cantemist/\",\n",
    "    model_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    dataset_name=\"Cantemist All\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
